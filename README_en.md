This README is written in English. For the French version, see [README.md](README.md).

# AeroStream â€“ Automated Customer Review Classification System

## Project Context

AeroStream aims to develop an intelligent system capable of automatically classifying customer reviews related to airline services. The main objective is to analyze customer satisfaction levels based on textual data extracted from user reviews.

This project combines NLP, machine learning, vector databases, and interactive visualization to provide real-time insights into customer satisfaction.

---

## Objectives

The system aims to:

- Collect and preprocess customer reviews
- Automatically analyze sentiment and satisfaction
- Generate performance indicators per airline
- Visualize results through an interactive dashboard

---

## Batch Pipeline

The batch pipeline is designed to prepare and process historical data and includes the following steps:

- **Data loading:** Import customer review datasets from Hugging Face
- **Exploratory Data Analysis (EDA):** Study class distributions, data distributions, and key statistics
- **Data cleaning:** Handle duplicates, missing values, and clean text data
- **Embedding generation:** Transform text into vector representations using NLP models (Sentence Transformers)
- **Metadata and embedding storage:** Store labels and vectors in a ChromaDB vector database, with separate collections for training and testing data
- **Model training and evaluation:** Train classification models and select the best-performing model for future predictions
- **Deployment via REST API:** Expose the trained model for production inference

---

## Streaming Pipeline

The streaming pipeline processes customer reviews in near real time:

- **Data ingestion:** Collect customer reviews via the API in micro-batches
- **Data preparation:** Clean and preprocess incoming reviews for sentiment prediction
- **Result storage:** Store predicted reviews in a PostgreSQL database
- **Aggregation and KPIs:**
    - Number of tweets per airline
    - Sentiment distribution per airline
    - Customer satisfaction rate per airline
    - Identification of the main causes of negative tweets
- **Visualization:** Interactive Streamlit dashboard displaying key KPIs, automatically updated with each data ingestion cycle
- **Automation:** Full pipeline orchestration using Airflow, scheduled to run every minute

---

## Technologies Used

- **Python** for data processing and machine learning
- **Docker** for application containerization
- **Sentence Transformers** for NLP
- **ChromaDB** for embedding management
- **PostgreSQL** for result storage
- **Streamlit** for the interactive dashboard
- **Airflow** for pipeline orchestration

---

## System Benefits

- Real-time customer satisfaction analysis
- Centralized management of historical and streaming data
- Clear and interactive visualization of key KPIs
- Fully automated pipeline for continuous customer feedback monitoring

---

## Installation

### Prerequisites

Make sure you have the following installed:

- Git
- Docker
- Docker Compose

### 1. Clone the repository

```Bash
git clone https://github.com/anass17/AeroStream.git
cd AeroStream
```

### 2. Generate the `.env` file

Create the `.env` file from the provided example:

```Bash
cp .env.example .env
```

Then open the `.env` file and adjust the variables if necessary.

### 3. Build and start Docker services

- Build Docker images:

```Bash
docker-compose build
```

- Start all services:

```Bash
docker-compose up -d
```

- Verify that containers are running correctly:

```Bash
docker-compose ps
```

### 4. Create the PostgreSQL database

- Access the PostgreSQL container:

```Bash
docker exec -it aerostream-postgres psql -U postgres
```

- Create the database:

```Bash
CREATE DATABASE aerostream;
```

- Verify database creation:

```Bash
\l
```

- Exit PostgreSQL:

```Bash
\q
```

### 5. Create an Airflow user

- Delete the default admin user:

```Bash
docker exec -it aerostream-airflow airflow users delete --username admin
```

- Create a new Airflow admin user:

```Bash
docker exec -it aerostream-airflow airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin --password admin
```

You may change the credentials according to your preferences.

### 6. Verify Airflow and DAGs

- Access the Airflow web interface:

```
http://localhost:8080
```

### 7. Verify the API

- Test that the API is accessible:

```Bash
curl http://localhost:8000/
```

- Access the API documentation:

```
http://localhost:8000/docs
```

### 8. Verify the Streamlit dashboard

- Access the dashboard:

```
http://localhost:8501
```

---

### Structure

```
ğŸ“ AeroStream
â”‚
â”œâ”€â”€ ğŸ“ notebooks
â”‚   â”œâ”€â”€ ğŸ“„ 01_loading.ipynb             # Load raw data
â”‚   â”œâ”€â”€ ğŸ“„ 02_eda.ipynb                 # Exploratory Data Analysis (EDA)
â”‚   â”œâ”€â”€ ğŸ“„ 03_preprocessing.ipynb       # Data cleaning and preprocessing
â”‚   â”œâ”€â”€ ğŸ“„ 04_embeddings.ipynb          # Embedding vector generation
â”‚   â”œâ”€â”€ ğŸ“„ 05_storage_chroma_db.ipynb   # Store embeddings in ChromaDB
â”‚   â””â”€â”€ ğŸ“„ 06_model_training.ipynb      # ML model training and evaluation
â”‚
â”œâ”€â”€ ğŸ“ data
â”‚   â”œâ”€â”€ ğŸ“ raw                          # Raw data
â”‚   â”œâ”€â”€ ğŸ“ processed                    # Cleaned and processed data
â”‚   â”œâ”€â”€ ğŸ“ embeddings                   # Embedding vectors
â”‚   â””â”€â”€ ğŸ“ chromaDB                     # Collections stored in ChromaDB
â”‚
â”œâ”€â”€ ğŸ“ api
â”‚   â””â”€â”€ ğŸ“ routers                      # API route definitions
â”‚
â”œâ”€â”€ ğŸ“ dashboard
â”‚   â”œâ”€â”€ ğŸ“ components                   # Reusable UI components
â”‚   â”œâ”€â”€ ğŸ“ extraction                   # Data extraction logic
â”‚   â”œâ”€â”€ ğŸ“ pages                        # Dashboard pages
â”‚   â””â”€â”€ ğŸ“„ app.py                       # Streamlit application entry point
â”‚
â”œâ”€â”€ ğŸ“ airflow
â”‚   â””â”€â”€ ğŸ“ dags
â”‚       â”œâ”€â”€ ğŸ“ tasks                    # Individual DAG tasks
â”‚       â””â”€â”€ ğŸ“„ aerostream_pipeline_dag.py # Main Airflow DAG
â”‚
â”œâ”€â”€ ğŸ“ models
â”‚   â”œâ”€â”€ ğŸ“ encoders                     # Encoder models
â”‚   â””â”€â”€ ğŸ“ ml                           # Machine learning models
â”‚
â”œâ”€â”€ ğŸ“ docker                           # Dockerfiles
â”œâ”€â”€ ğŸ“ docs                             # Project documentation
â”œâ”€â”€ ğŸ“ pgdata                           # PostgreSQL data files
â”œâ”€â”€ ğŸ“ requirements                    # Dependencies for each Docker service
â”œâ”€â”€ ğŸ“„ docker-compose.yml               # Docker Compose configuration
â”œâ”€â”€ ğŸ“„ .env                             # Environment variables
â”œâ”€â”€ ğŸ“„ README.md                        # Project documentation (French)
â”œâ”€â”€ ğŸ“„ README_en.md                     # Project documentation (English)
â””â”€â”€ ğŸ“„ .gitignore                       # Git ignore rules
```

---

## Visualisations

### Interface Streamlit

![Streamlit UI](https://github.com/user-attachments/assets/883039ab-2919-4c9c-a29d-1508a40f5d3e)

### Interface Airflow

![Airflow UI](https://github.com/user-attachments/assets/1ffe1001-1031-426d-b6ac-cc71b4637287)

### API Endpoints

![API](https://github.com/user-attachments/assets/b81946b5-9c6d-42f7-8a25-7b418d566255)

### Architecture

![Architecture](https://github.com/user-attachments/assets/3841e38b-63cd-468e-870d-42222abf5419)